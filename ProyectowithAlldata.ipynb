{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = 'full_format_recipes.json' \n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (20130, 11)\n",
      "Columns: Index(['directions', 'fat', 'date', 'categories', 'calories', 'desc',\n",
      "       'protein', 'rating', 'title', 'ingredients', 'sodium'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic exploration\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(\"Columns:\", data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on relevant columns\n",
    "StringData = data[['directions', 'desc', 'rating','categories','title']].copy()\n",
    "StringData.dropna(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "0.000    1153\n",
      "1.250     113\n",
      "1.875      70\n",
      "2.500     360\n",
      "3.125     979\n",
      "3.750    3322\n",
      "4.375    5438\n",
      "5.000    2049\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyze relationships ##TODO: SMTH better than this shit\n",
    "print(StringData.groupby('rating').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "StringData['processed_directions'] = StringData['directions'].apply(lambda x: preprocess_text(\" \".join(x) if isinstance(x, list) else x))\n",
    "StringData['processed_desc'] = StringData['desc'].apply(lambda x: preprocess_text(\" \".join(x) if isinstance(x, list) else x))\n",
    "StringData['processed_categories'] = StringData['categories'].apply(lambda x: preprocess_text(\" \".join(x) if isinstance(x, list) else x))\n",
    "StringData['processed_title'] = StringData['title'].apply(lambda x: preprocess_text(\" \".join(x) if isinstance(x, list) else x))\n",
    "\n",
    "# TF-IDF vectorization for individual columns\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "categories_tfidf = vectorizer.fit_transform(data['processed_categories'])\n",
    "desc_tfidf = vectorizer.fit_transform(data['processed_desc'])\n",
    "directions_tfidf = vectorizer.fit_transform(data['processed_directions'])\n",
    "title_tfidf = vectorizer.fit_transform(data['processed_title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                processed_directions  \\\n",
      "1  combine ingredient heavy medium saucepan add s...   \n",
      "3  heat oil heavy large skillet medium high heat ...   \n",
      "5  Mix basil mayonnaise butter processor basil fi...   \n",
      "6  cook potato carrot large pot boiling salt wate...   \n",
      "7  stir sugar chili powder whisk egg white water ...   \n",
      "\n",
      "                                      processed_desc  \\\n",
      "1  use ingredient find boudin blanc classic frenc...   \n",
      "3  sicilian style tomato sauce ton Mediterranean ...   \n",
      "5                              recipe prepare minute   \n",
      "6  serve newfangle main course salad crisp flatbr...   \n",
      "7                 pop mouth burst bittersweet flavor   \n",
      "\n",
      "                                processed_categories  \\\n",
      "1  Food Processor Onion Pork Bake Bastille Day Ne...   \n",
      "3  fish Olive Tomato SautÃ© Low Fat Low Cal High F...   \n",
      "5  Sandwich Food Processor Tomato kid Friendly Qu...   \n",
      "6  Salad Potato Easter Low Fat Quick Easy Ham Asp...   \n",
      "7  Egg Fruit Cook Cocktail Party Vegetarian Winte...   \n",
      "\n",
      "                                  processed_title  \n",
      "1           Boudin Blanc Terrine Red Onion Confit  \n",
      "3                    Mahi Mahi Tomato Olive Sauce  \n",
      "5                                       Best Blts  \n",
      "6  ham Spring Vegetable Salad Shallot Vinaigrette  \n",
      "7                            Spicy Sweet Kumquats  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display a few samples of the processed data\n",
    "print(StringData[['processed_directions', 'processed_desc', 'processed_categories', 'processed_title']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features into a single DataFrame\n",
    "numeric_features = data[['fat', 'protein', 'calories', 'sodium']]\n",
    "scaler = StandardScaler()\n",
    "numeric_features = scaler.fit_transform(numeric_features)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Use top 5000 features\n",
    "categories_tfidf = vectorizer.fit_transform(StringData['processed_categories'])\n",
    "desc_tfidf = vectorizer.fit_transform(StringData['processed_desc'])\n",
    "directions_tfidf = vectorizer.fit_transform(StringData['processed_directions'])\n",
    "title_tfidf = vectorizer.fit_transform(StringData['processed_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data to X and Y\n",
    "X = torch.tensor(\n",
    "    pd.concat(\n",
    "        [pd.DataFrame(categories_tfidf.toarray()), pd.DataFrame(desc_tfidf.toarray()), pd.DataFrame(directions_tfidf.toarray()), pd.DataFrame(title_tfidf.toarray()), pd.DataFrame(numeric_features)],\n",
    "        axis=1\n",
    "    ).values,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "y = torch.tensor(data['rating'].values, dtype=torch.float32).unsqueeze(1)  # Target variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Representation\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot delete function call (261873487.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    del RandomForestRegressor()\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot delete function call\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RandomForestRegressor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m RF \u001b[38;5;241m=\u001b[39m \u001b[43mRandomForestRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m RF\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'RandomForestRegressor' object is not callable"
     ]
    }
   ],
   "source": [
    "RF = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "RF.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MSE: 1.3997153112293332\n",
      "Random Forest R2 Score: 0.1760447856207914\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predictions and evaluation\n",
    "rf_predictions = rf_pipeline.predict(X_test)\n",
    "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf_predictions))\n",
    "print(\"Random Forest R2 Score:\", r2_score(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using BERT embeddings for contextual embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
    "    outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate BERT embeddings for a small subset (for demonstration purposes)##TODO:Don't preprocess\n",
    "#StringData_reduced100 = StringData[:100]  # Use smaller subset to avoid memory issues\n",
    "sample_embeddings = np.vstack(StringData['processed_directions'].apply(get_bert_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split embeddings\n",
    "y_sample = StringData['rating']\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(\n",
    "    sample_embeddings, y_sample, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a neural network using PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "input_size = sample_embeddings.shape[1]\n",
    "model = SimpleNN(input_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train_bert, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_bert.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_bert, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_bert.values, dtype=torch.float32).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.933030128479004\n",
      "Epoch 20, Loss: 2.492696523666382\n",
      "Epoch 30, Loss: 2.001760959625244\n",
      "Epoch 40, Loss: 1.7109516859054565\n",
      "Epoch 50, Loss: 1.6753886938095093\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_train_tensor)\n",
    "    loss = criterion(predictions, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Test Loss: 1.9380100965499878\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(test_predictions, y_test_tensor)\n",
    "    print(\"Neural Network Test Loss:\", test_loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
