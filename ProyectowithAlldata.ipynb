{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = 'full_format_recipes.json' \n",
    "data = pd.read_json(file_path)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (10608, 11)\n",
      "Columns: Index(['directions', 'fat', 'date', 'categories', 'calories', 'desc',\n",
      "       'protein', 'rating', 'title', 'ingredients', 'sodium'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic exploration\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(\"Columns:\", data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on relevant columns\n",
    "StringData = data[['directions', 'desc', 'rating','categories','title']].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "0.000     809\n",
      "1.250      87\n",
      "1.875      40\n",
      "2.500     270\n",
      "3.125     765\n",
      "3.750    2649\n",
      "4.375    4423\n",
      "5.000    1565\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyze relationships ##TODO: SMTH better than this shit\n",
    "print(StringData.groupby('rating').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "StringData['processed_directions'] = StringData['directions'].apply(lambda x: preprocess_text(\" \".join(x) if isinstance(x, list) else x))\n",
    "StringData['processed_desc'] = StringData['desc'].apply(lambda x: preprocess_text(\" \".join(x) if isinstance(x, list) else x))\n",
    "StringData['processed_categories'] = StringData['categories'].apply(lambda x: preprocess_text(\" \".join(x) if isinstance(x, list) else x))\n",
    "StringData['processed_title'] = StringData['title'].apply(lambda x: preprocess_text(\" \".join(x) if isinstance(x, list) else x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 processed_directions  \\\n",
      "1   combine ingredient heavy medium saucepan add s...   \n",
      "5   Mix basil mayonnaise butter processor basil fi...   \n",
      "8   stir soy sauce sugar sesame oil white pale gre...   \n",
      "9   Chop parsley leave measure tablespoon reserve ...   \n",
      "10  heat oil heavy large skillet medium high heat ...   \n",
      "\n",
      "                                       processed_desc  \\\n",
      "1   use ingredient find boudin blanc classic frenc...   \n",
      "5                               recipe prepare minute   \n",
      "8                                             Bulgogi   \n",
      "9   transform picnic un pique nique remember elega...   \n",
      "10        simmer yam fill flavor yield lovely coating   \n",
      "\n",
      "                                 processed_categories  \\\n",
      "1   Food Processor Onion Pork Bake Bastille Day Ne...   \n",
      "5   Sandwich Food Processor Tomato kid Friendly Qu...   \n",
      "8   Beef Ginger Saut√© Stir Fry Quick Easy Spring S...   \n",
      "9   Salad Mustard Potato Picnic Lunch Mayonnaise H...   \n",
      "10  Milk Cream Dairy Thanksgiving Rosemary Sweet P...   \n",
      "\n",
      "                                    processed_title  \n",
      "1             Boudin Blanc Terrine Red Onion Confit  \n",
      "5                                         Best Blts  \n",
      "8                             korean Marinated Beef  \n",
      "9   Ham Persillade Mustard Potato Salad Mashed Peas  \n",
      "10                 yam braise Cream Rosemary Nutmeg  \n"
     ]
    }
   ],
   "source": [
    "print(StringData[['processed_directions', 'processed_desc', 'processed_categories', 'processed_title']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization for individual columns\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "categories_tfidf = vectorizer.fit_transform(StringData['processed_categories'])\n",
    "desc_tfidf = vectorizer.fit_transform(StringData['processed_desc'])\n",
    "directions_tfidf = vectorizer.fit_transform(StringData['processed_directions'])\n",
    "title_tfidf = vectorizer.fit_transform(StringData['processed_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features into a single DataFrame\n",
    "numeric_features = data[['fat', 'protein', 'calories', 'sodium']].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numeric_features = scaler.fit_transform(numeric_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data to X and Y\n",
    "X = torch.tensor(\n",
    "    pd.concat(\n",
    "        [pd.DataFrame(categories_tfidf.toarray()), pd.DataFrame(desc_tfidf.toarray()), pd.DataFrame(directions_tfidf.toarray()), pd.DataFrame(title_tfidf.toarray()), pd.DataFrame(numeric_features)],\n",
    "        axis=1\n",
    "    ).values,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "y = torch.tensor(data['rating'].values, dtype=torch.float32)  # Target variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "RF.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predictions and evaluation\n",
    "rf_predictions = rf_pipeline.predict(X_test)\n",
    "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf_predictions))\n",
    "print(\"Random Forest R2 Score:\", r2_score(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using BERT embeddings for contextual embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
    "    outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate BERT embeddings for a small subset (for demonstration purposes)##TODO:Don't preprocess\n",
    "#StringData_reduced100 = StringData[:100]  # Use smaller subset to avoid memory issues\n",
    "sample_embeddings = np.vstack(StringData['processed_directions'].apply(get_bert_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split embeddings\n",
    "y_sample = StringData['rating']\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(\n",
    "    sample_embeddings, y_sample, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a neural network using PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "input_size = sample_embeddings.shape[1]\n",
    "model = SimpleNN(input_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train_bert, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_bert.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_bert, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_bert.values, dtype=torch.float32).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_train_tensor)\n",
    "    loss = criterion(predictions, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(test_predictions, y_test_tensor)\n",
    "    print(\"Neural Network Test Loss:\", test_loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
