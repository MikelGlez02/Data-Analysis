{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from torch.hub import load\n",
    "from gensim.models import Word2Vec\n",
    "import dill\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = 'full_format_recipes.json' \n",
    "data = pd.read_json(file_path)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic exploration\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(\"Columns:\", data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on relevant columns\n",
    "StringData = data[['directions', 'desc', 'rating','categories','title']].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Explode the 'categories' column and calculate mean ratings for each category\n",
    "categories_exploded = data.explode('categories')\n",
    "category_ratings = categories_exploded.groupby('categories')['rating'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Plot the top 20 categories\n",
    "plt.figure(figsize=(12, 8))\n",
    "category_ratings.head(20).plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 20 Categorías Más Valoradas', fontsize=16)\n",
    "plt.xlabel('Categoría', fontsize=14)\n",
    "plt.ylabel('Puntuación promedio', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot all categories without x-axis labels\n",
    "plt.figure(figsize=(16, 10))\n",
    "category_ratings.plot(kind='bar', color='lightgreen')\n",
    "plt.title('Puntuaciones Promedio de Todas las Categorías', fontsize=16)\n",
    "plt.xlabel('Categoría', fontsize=14)\n",
    "plt.ylabel('Puntuación promedio', fontsize=14)\n",
    "plt.xticks([])  # Removes x-axis labels\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the total number of unique categories\n",
    "categorias_unicas = categories_exploded['categories'].nunique()\n",
    "print(f\"Número total de categorías únicas: {categorias_unicas}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "StringData['processed_directions'] = StringData['directions'].apply(lambda x: preprocess_text(\" \".join(x) if isinstance(x, list) else x))\n",
    "StringData['processed_desc'] = StringData['desc'].apply(lambda x: preprocess_text(\" \".join(x) if isinstance(x, list) else x))\n",
    "StringData['processed_categories'] = StringData['categories'].apply(lambda x: preprocess_text(\" \".join(x) if isinstance(x, list) else x))\n",
    "StringData['processed_title'] = StringData['title'].apply(lambda x: preprocess_text(\" \".join(x) if isinstance(x, list) else x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and normalize numeric features\n",
    "numeric_features = data[['fat', 'protein', 'calories', 'sodium']]\n",
    "scaler = StandardScaler()\n",
    "numeric_features = scaler.fit_transform(numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(StringData[['processed_directions', 'processed_desc', 'processed_categories', 'processed_title']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------TF-IDF------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization for individual columns\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "categories_tfidf = vectorizer.fit_transform(StringData['processed_categories'])\n",
    "desc_tfidf = vectorizer.fit_transform(StringData['processed_desc'])\n",
    "directions_tfidf = vectorizer.fit_transform(StringData['processed_directions'])\n",
    "title_tfidf = vectorizer.fit_transform(StringData['processed_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data to X and Y combining numeric and written features\n",
    "X_TFIDF = torch.tensor(\n",
    "    pd.concat(\n",
    "        [pd.DataFrame(categories_tfidf.toarray()), pd.DataFrame(desc_tfidf.toarray()), pd.DataFrame(directions_tfidf.toarray()), pd.DataFrame(title_tfidf.toarray()), pd.DataFrame(numeric_features)],\n",
    "        axis=1\n",
    "    ).values,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "y = torch.tensor(data['rating'].values, dtype=torch.float32)  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train_TFIDF, X_test_TFIDF, y_train, y_test = train_test_split(\n",
    "   X_TFIDF, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------WORD2VEC------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Word2Vec\n",
    "documents = StringData['processed_directions'].tolist() + StringData['processed_desc'].tolist() + \\\n",
    "            StringData['processed_categories'].tolist() + StringData['processed_title'].tolist()\n",
    "tokenized_documents = [doc.split() for doc in documents]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate document embeddings\n",
    "def get_doc_embedding(doc, model):\n",
    "    words = doc.split()\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vectors) > 0:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate Word2Vec embeddings for each column\n",
    "StringData['directions_w2v'] = StringData['processed_directions'].apply(lambda x: get_doc_embedding(x, word2vec_model))\n",
    "StringData['desc_w2v'] = StringData['processed_desc'].apply(lambda x: get_doc_embedding(x, word2vec_model))\n",
    "StringData['categories_w2v'] = StringData['processed_categories'].apply(lambda x: get_doc_embedding(x, word2vec_model))\n",
    "StringData['title_w2v'] = StringData['processed_title'].apply(lambda x: get_doc_embedding(x, word2vec_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine Word2Vec embeddings into a single feature matrix\n",
    "w2v_features = np.array(StringData['directions_w2v'].tolist()) + \\\n",
    "               np.array(StringData['desc_w2v'].tolist()) + \\\n",
    "               np.array(StringData['categories_w2v'].tolist()) + \\\n",
    "               np.array(StringData['title_w2v'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data to X and Y combining numeric and written features\n",
    "X_W2V = torch.tensor(\n",
    "    pd.concat(\n",
    "        [pd.DataFrame(w2v_features.toarray()), pd.DataFrame(numeric_features)],\n",
    "        axis=1\n",
    "    ).values,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "y = torch.tensor(data['rating'].values, dtype=torch.float32)  # Target variable\n",
    "\n",
    "#\n",
    "# final_features = np.hstack([w2v_features, numeric_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train_W2V, X_test_W2V, y_train, y_test = train_test_split(\n",
    "   X_W2V, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------BERT------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using BERT embeddings for contextual embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate BERT embeddings for a small subset (for demonstration purposes)\n",
    "#StringData_reduced100 = StringData[:100]  # Use smaller subset to avoid memory issues\n",
    "\n",
    "# Compute BERT embeddings for all processed text columns\n",
    "embeddings_directions = np.vstack(StringData['directions'].apply(get_bert_embedding))\n",
    "embeddings_desc = np.vstack(StringData['desc'].apply(get_bert_embedding))\n",
    "embeddings_categories = np.vstack(StringData['categories'].apply(get_bert_embedding))\n",
    "embeddings_title = np.vstack(StringData['title'].apply(get_bert_embedding))\n",
    "# Combine BERT embeddings and numeric features\n",
    "\n",
    "X_BERT = torch.tensor(\n",
    "    pd.concat(\n",
    "        [pd.DataFrame(embeddings_directions.toarray()),pd.DataFrame(embeddings_desc.toarray()),pd.DataFrame(embeddings_categories.toarray()),pd.DataFrame(embeddings_title.toarray()), pd.DataFrame(numeric_features)],\n",
    "        axis=1\n",
    "    ).values,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "y = torch.tensor(data['rating'].values, dtype=torch.float32)  # Target variable\n",
    "\n",
    "\n",
    "# Stack all features together\n",
    "#sample_embeddings = np.hstack([\n",
    "#    embeddings_directions,\n",
    "#    embeddings_desc,\n",
    "#    embeddings_categories,\n",
    "#    embeddings_title,\n",
    "#    numeric_features\n",
    "#])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split embeddings\n",
    "X_train_bert, X_test_bert, y_train, y_test = train_test_split(\n",
    "    X_BERT, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------KNN------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_model.fit(X_train_TFIDF, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X_test and y_test are numpy arrays\n",
    "X_test_TFIDF_np = X_train_TFIDF.cpu().numpy() if isinstance(X_train_TFIDF, torch.Tensor) else X_train_TFIDF\n",
    "y_test_np = y_test.cpu().numpy() if isinstance(y_test, torch.Tensor) else y_test\n",
    "\n",
    "# Predictions and evaluation\n",
    "knn_predictions = knn_model.predict(X_test_TFIDF)\n",
    "print(\"KNN MSE:\", mean_squared_error(y_test, knn_predictions))\n",
    "print(\"KNN MAE:\", np.mean(np.abs(y_test - knn_predictions)))\n",
    "print(\"KNN R2 Score:\", r2_score(y_test, knn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------SIMPLE NN------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a neural network using PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "input_size = X_train_bert.shape[1]\n",
    "model = SimpleNN(input_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train_bert, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_bert, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_train_tensor)\n",
    "    loss = criterion(predictions, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(test_predictions, y_test_tensor)\n",
    "    print(\"Neural Network Test Loss:\", test_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, text_input_size, numeric_input_size):\n",
    "        super(TabularModel, self).__init__()\n",
    "        self.text_branch = nn.Sequential(\n",
    "            nn.Linear(text_input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.numeric_branch = nn.Sequential(\n",
    "            nn.Linear(numeric_input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.combined_branch = nn.Sequential(\n",
    "            nn.Linear(128 + 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, text_features, numeric_features):\n",
    "        text_out = self.text_branch(text_features)\n",
    "        numeric_out = self.numeric_branch(numeric_features)\n",
    "        combined = torch.cat([text_out, numeric_out], dim=1)\n",
    "        return self.combined_branch(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine numeric features and BERT embeddings\n",
    "text_dim = embeddings_directions.shape[1] * 4  # Combined text embeddings\n",
    "numeric_dim = numeric_features.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_bert[:, :text_dim], dtype=torch.float32).to(device)\n",
    "X_train_numeric_tensor = torch.tensor(X_train_bert[:, text_dim:], dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_train_bert[:, :text_dim], dtype=torch.float32).to(device)\n",
    "X_test_numeric_tensor = torch.tensor(X_train_bert[:, text_dim:], dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the custom TabularModel\n",
    "model = TabularModel(text_input_size=text_dim, numeric_input_size=numeric_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_train_tensor, X_train_numeric_tensor)\n",
    "    loss = criterion(predictions, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor, X_test_numeric_tensor)\n",
    "    test_loss = criterion(test_predictions, y_test_tensor)\n",
    "    print(\"Tabular Neural Network Test Loss:\", test_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dill.dump_session('notebook_env.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
